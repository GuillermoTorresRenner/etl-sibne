#!/usr/bin/env node

/**
 * SIBNE ETL - Script de Migraci√≥n Completa con Extracci√≥n
 *
 * Script realiza todo el proceso completo:
 * 1. Extrae datos desde SQL Server a CSV
 * 2. Analiza esquemas Prisma
 * 3. Ejecuta migraci√≥n ordenada a PostgreSQL
 * 4. Procesa archivos binarios
 */

import { fileURLToPath } from "url";
import path from "path";
import fs from "fs";
import sql from "mssql";
import dotenv from "dotenv";
import { logger } from "./src/utils/logger.js";
import MigrationReportGenerator from "./src/scripts/generate-migration-report.js";

dotenv.config();

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Configuraci√≥n de SQL Server
const sqlConfig = {
  user: "sa",
  password: process.env.SA_PASSWORD,
  server: process.env.SQL_HOST,
  port: parseInt(process.env.SQL_PORT),
  database: process.env.DB_NAME,
  options: {
    encrypt: false,
    trustServerCertificate: true,
  },
};

// Lista de tablas a extraer (en orden alfab√©tico para simplificar)
const ALL_TABLES = [
  "ArchivoAdjunto",
  "CargaMasivaArchivo",
  "CargaMasivaDetalle",
  "CargaMasivaError",
  "CategoriaTransaccion",
  "Comuna",
  "Contacto",
  "EmailConfig",
  "EmailLogs",
  "Empresa",
  "Encuesta",
  "EncuestaEmpresa",
  "EncuestaPlanta",
  "Energetico",
  "EnergeticoGrupos",
  "EnergeticoUnidadMedida",
  "EstadoEmail",
  "EstadoEmpresa",
  "EstadoProceso",
  "EstadoReporte",
  "FactorConversion",
  "Formulario",
  "IntensidadEnergEncuestaEmpresa",
  "Pais",
  "Planta",
  "Provincia",
  "Region",
  "Reporte",
  "Role",
  "SectorEconomico",
  "SectorEconomicoSii",
  "SectorEnergetico",
  "SubSectorEconomico",
  "Tecnologia",
  "TipoContacto",
  "TipoOtroUso",
  "TipoPerdida",
  "TipoTransporte",
  "TipoUsoProceso",
  "Transaccion",
  "TransaccionIntensidadEnergia",
  "UnidadMedida",
  "Users",
  "Usuario",
  "UsuarioLogin",
  "UsuarioRole",
  "UsuarioToken",
];

// ÔøΩ MAPEO DE NOMBRES DE TABLAS
// Mapeo de nombres Prisma (destino) -> nombres SQL Server (origen)
const TABLE_NAME_MAPPING = {
  "Usuario": "AspNetUsers",
  "Role": "AspNetRoles", 
  "UsuarioLogin": "AspNetUserLogins",
  "UsuarioToken": "AspNetUserTokens",
  // NOTA: UsuarioRole NO se incluye porque Prisma maneja la relaci√≥n many-to-many autom√°ticamente
  // Los datos de AspNetUserRoles se migran a trav√©s de la tabla impl√≠cita _UsuarioRoles que crea Prisma
};

// ÔøΩüöß TABLAS QUE NO EXISTEN EN LA BD ORIGINAL
// Estas tablas NO est√°n en la base de datos de SQL Server
const DEV_TABLES_TO_EXCLUDE = [
  "Users", // Tabla obsoleta
  "Encuesta",
  "EncuestaEmpresa", 
  "EncuestaPlanta",
  "IntensidadEnergEncuestaEmpresa",
];

// Filtrar tablas que realmente existen en la BD original
const TABLES_TO_EXTRACT = ALL_TABLES.filter(
  (table) => !DEV_TABLES_TO_EXCLUDE.includes(table)
);

// Funci√≥n para extraer tabla a CSV
async function extractTableToCSV(sqlPool, tableName) {
  try {
    // Usar el mapeo de nombres si existe, sino usar el nombre original
    const sourceTableName = TABLE_NAME_MAPPING[tableName] || tableName;
    const tableInfo = sourceTableName !== tableName ? `${tableName} (${sourceTableName})` : tableName;
    const query = `SELECT * FROM dbo.${sourceTableName}`;
    const result = await sqlPool.request().query(query);

    if (result.recordset.length === 0) {
      console.log(`‚ö†Ô∏è  Tabla ${tableInfo}: Sin datos`);
      logger.warn(`Tabla ${tableInfo} sin datos`);
      return false;
    }

    // Crear CSV
    const headers = Object.keys(result.recordset[0]);
    const csvContent = [
      headers.join(","),
      ...result.recordset.map((row) =>
        headers
          .map((header) => {
            let value = row[header];
            // Manejar valores especiales
            if (value === null || value === undefined) {
              return "";
            }
            if (
              typeof value === "string" &&
              (value.includes(",") || value.includes('"'))
            ) {
              return `"${value.replace(/"/g, '""')}"`;
            }
            if (value instanceof Date) {
              return value.toISOString();
            }
            if (Buffer.isBuffer(value)) {
              return ""; // Omitir binarios en CSV normal
            }
            return value.toString();
          })
          .join(",")
      ),
    ].join("\n");

    // Guardar archivo
    const csvPath = path.join(__dirname, "Tablas", `dbo_${tableName}.csv`);
    fs.writeFileSync(csvPath, csvContent, "utf8");

    console.log(
      `‚úÖ ${tableInfo}: ${result.recordset.length} registros extra√≠dos`
    );
    logger.info(
      `Tabla ${tableInfo} extra√≠da: ${result.recordset.length} registros`
    );
    return true;
  } catch (error) {
    console.error(`‚ùå Error extrayendo tabla ${tableName}:`, error.message);
    logger.error(`Error extrayendo tabla ${tableName}`, error);
    return false;
  }
}

// Funci√≥n principal
async function runFullMigration() {
  const startTime = Date.now();
  let sqlPool;

  try {
    const logMessage = "üöÄ SIBNE ETL - MIGRACI√ìN COMPLETA CON EXTRACCI√ìN";
    console.log(logMessage);
    console.log("=".repeat(60));
    logger.info(logMessage);
    logger.info("Iniciando proceso de migraci√≥n completa");

    // Mostrar informaci√≥n sobre tablas excluidas
    if (DEV_TABLES_TO_EXCLUDE.length > 0) {
      console.log(
        `\nüöß Tablas de desarrollo excluidas: ${DEV_TABLES_TO_EXCLUDE.length}`
      );
      DEV_TABLES_TO_EXCLUDE.forEach((table) => {
        console.log(`   - ${table} (no existe en BD original)`);
      });
      logger.info(
        `Tablas de desarrollo excluidas: ${DEV_TABLES_TO_EXCLUDE.join(", ")}`
      );
    }

    // PASO 1: Conectar a SQL Server y extraer datos
    console.log("\nüìä PASO 1: Extrayendo datos desde SQL Server...\n");
    logger.info("PASO 1: Iniciando extracci√≥n de datos desde SQL Server");

    sqlPool = await sql.connect(sqlConfig);
    console.log("‚úÖ Conectado a SQL Server");
    logger.info("Conexi√≥n establecida con SQL Server");

    let extractedCount = 0;
    let failedCount = 0;

    // Asegurar que existe la carpeta Tablas
    const tablasDir = path.join(__dirname, "Tablas");
    if (!fs.existsSync(tablasDir)) {
      fs.mkdirSync(tablasDir, { recursive: true });
    }

    // Extraer cada tabla
    for (const tableName of TABLES_TO_EXTRACT) {
      const success = await extractTableToCSV(sqlPool, tableName);
      if (success) {
        extractedCount++;
      } else {
        failedCount++;
      }
    }

    console.log(`\nüìà Extracci√≥n completada:`);
    console.log(`   ‚úÖ Tablas extra√≠das: ${extractedCount}`);
    console.log(`   ‚ùå Tablas fallidas: ${failedCount}`);

    logger.info(
      `Extracci√≥n completada: ${extractedCount} exitosas, ${failedCount} fallidas`
    );

    // Cerrar conexi√≥n SQL Server
    await sqlPool.close();
    logger.info("Conexi√≥n SQL Server cerrada");

    // PASO 2: Analizar esquemas Prisma
    console.log("\nüîç PASO 2: Analizando esquemas Prisma...\n");
    logger.info("PASO 2: Iniciando an√°lisis de esquemas Prisma");

    const { default: analyzePrismaSchema } = await import(
      "./src/analyzers/analyze-prisma-schema.js"
    );
    // El analizador se ejecuta autom√°ticamente al importarse
    logger.info("An√°lisis de esquemas Prisma completado");

    // PASO 3: Ejecutar migraci√≥n a PostgreSQL
    console.log("\nüîÑ PASO 3: Ejecutando migraci√≥n a PostgreSQL...\n");
    logger.info("PASO 3: Iniciando migraci√≥n a PostgreSQL");

    const { default: runPrismaMigration } = await import(
      "./src/migrations/final-prisma-migration.js"
    );
    await runPrismaMigration();
    logger.info("Migraci√≥n a PostgreSQL completada");

    // PASO 4: Procesar archivos binarios
    console.log("\nüìÅ PASO 4: Procesando archivos binarios...\n");
    logger.info("PASO 4: Iniciando procesamiento de archivos binarios");

    const { default: extractArchivoAdjunto } = await import(
      "./src/processors/extract-archivo-adjunto.js"
    );
    await extractArchivoAdjunto();
    logger.info("Procesamiento de archivos binarios completado");

    // PASO 5: Generar reporte final
    const endTime = Date.now();
    const duration = Math.round((endTime - startTime) / 1000);

    console.log("\nüìã PASO 5: Generando reporte final...\n");
    logger.info("PASO 5: Generando reporte de migraci√≥n");

    try {
      const reportGenerator = new MigrationReportGenerator();
      const reportPath = await reportGenerator.generateReport({
        extractedCount,
        failedCount,
        duration,
        totalTables: TABLES_TO_EXTRACT.length,
      });

      console.log(`‚úÖ Reporte generado: ${reportPath}`);
      logger.info(`Reporte de migraci√≥n generado en: ${reportPath}`);
    } catch (reportError) {
      console.error("‚ö†Ô∏è  Error generando reporte:", reportError.message);
      logger.error("Error generando reporte:", reportError);
    }

    console.log("\nüéâ ¬°MIGRACI√ìN COMPLETA FINALIZADA EXITOSAMENTE!");
    console.log("=".repeat(60));
    console.log("üìä Resumen final:");
    console.log(`   - Tablas extra√≠das desde SQL Server: ${extractedCount}`);
    console.log("   - Esquemas Prisma analizados: ‚úÖ");
    console.log("   - Migraci√≥n PostgreSQL: ‚úÖ");
    console.log("   - Archivos binarios procesados: ‚úÖ");
    console.log(`   - Duraci√≥n total: ${duration} segundos`);

    logger.info(
      `Migraci√≥n completa finalizada exitosamente en ${duration} segundos`
    );
    logger.info(
      `Resumen: ${extractedCount} tablas extra√≠das, ${failedCount} errores`
    );
  } catch (error) {
    console.error("\nüí• ERROR en la migraci√≥n completa:", error.message);
    console.error("Stack trace:", error.stack);
    logger.error("Error cr√≠tico en migraci√≥n completa", error);

    // Generar reporte de error
    try {
      const reportGenerator = new MigrationReportGenerator();
      const errorReportPath = await reportGenerator.generateErrorReport(error);
      console.log(`üìã Reporte de error generado: ${errorReportPath}`);
      logger.info(`Reporte de error generado en: ${errorReportPath}`);
    } catch (reportError) {
      logger.error("Error generando reporte de error:", reportError);
    }

    process.exit(1);
  } finally {
    if (sqlPool) {
      await sqlPool.close();
      logger.info("Conexi√≥n SQL Server cerrada en cleanup");
    }
  }
}

// Ejecutar si es llamado directamente
if (import.meta.url === `file://${process.argv[1]}`) {
  runFullMigration();
}

export default runFullMigration;
